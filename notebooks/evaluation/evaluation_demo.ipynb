{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentinella Evaluation Demo\n",
        "\n",
        "This notebook demonstrates the evaluation pipeline for comparing LLM models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import pandas as pd\n",
        "from src.evaluator.evaluator import Evaluator\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = Evaluator(gateway_url=\"http://localhost:8000\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Golden Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path = \"src/evaluator/datasets/golden_dataset.json\"\n",
        "\n",
        "with open(dataset_path, \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(dataset)} test cases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Single Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate GPT-3.5 Turbo\n",
        "results = await evaluator.evaluate_model(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    dataset_path=dataset_path,\n",
        "    output_path=\"evaluation_results_gpt35.json\"\n",
        ")\n",
        "\n",
        "print(f\"Average Quality Score: {results['average_quality_score']:.2%}\")\n",
        "print(f\"Average Latency: {results['average_latency_ms']:.2f}ms\")\n",
        "print(f\"Total Cost: ${results['total_cost']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Multiple Models (A/B Testing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare GPT-4o-mini vs GPT-3.5-turbo vs Claude Haiku\n",
        "comparison = await evaluator.compare_models(\n",
        "    models=[\"gpt-4o-mini\", \"gpt-3.5-turbo\", \"claude-3-haiku\"],\n",
        "    dataset_path=dataset_path\n",
        ")\n",
        "\n",
        "# Display comparison\n",
        "comparison_df = pd.DataFrame({\n",
        "    model: {\n",
        "        \"Quality Score\": comp[\"average_quality_score\"],\n",
        "        \"Latency (ms)\": comp[\"average_latency_ms\"],\n",
        "        \"Cost\": comp[\"total_cost\"],\n",
        "    }\n",
        "    for model, comp in comparison[\"detailed_results\"].items()\n",
        "})\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(comparison_df.T)\n",
        "print(f\"\\nBest Quality: {comparison['best_quality']}\")\n",
        "print(f\"Fastest: {comparison['fastest']}\")\n",
        "print(f\"Cheapest: {comparison['cheapest']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create comparison charts\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "models = list(comparison[\"detailed_results\"].keys())\n",
        "quality_scores = [comparison[\"detailed_results\"][m][\"average_quality_score\"] for m in models]\n",
        "latencies = [comparison[\"detailed_results\"][m][\"average_latency_ms\"] for m in models]\n",
        "costs = [comparison[\"detailed_results\"][m][\"total_cost\"] for m in models]\n",
        "\n",
        "axes[0].bar(models, quality_scores, color=['#4CAF50', '#2196F3', '#FF9800'])\n",
        "axes[0].set_title(\"Quality Scores\", fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel(\"Score (0-1)\")\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(quality_scores):\n",
        "    axes[0].text(i, v + 0.02, f'{v:.2%}', ha='center', va='bottom')\n",
        "\n",
        "axes[1].bar(models, latencies, color=['#4CAF50', '#2196F3', '#FF9800'])\n",
        "axes[1].set_title(\"Average Latency\", fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel(\"Latency (ms)\")\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(latencies):\n",
        "    axes[1].text(i, v + max(latencies)*0.02, f'{v:.0f}ms', ha='center', va='bottom')\n",
        "\n",
        "axes[2].bar(models, costs, color=['#4CAF50', '#2196F3', '#FF9800'])\n",
        "axes[2].set_title(\"Total Cost\", fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel(\"Cost (USD)\")\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(costs):\n",
        "    axes[2].text(i, v + max(costs)*0.02, f'${v:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display detailed results for each model\n",
        "for model_name, model_results in comparison[\"detailed_results\"].items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total Tests: {model_results['total_tests']}\")\n",
        "    print(f\"Successful: {model_results['successful']}\")\n",
        "    print(f\"Success Rate: {model_results['successful']/model_results['total_tests']:.2%}\")\n",
        "    print(f\"Average Quality Score: {model_results['average_quality_score']:.2%}\")\n",
        "    print(f\"Average Latency: {model_results['average_latency_ms']:.2f}ms\")\n",
        "    print(f\"Total Cost: ${model_results['total_cost']:.4f}\")\n",
        "    \n",
        "    # Show sample results\n",
        "    print(f\"\\nSample Results:\")\n",
        "    for i, result in enumerate(model_results['results'][:3]):\n",
        "        if 'error' not in result:\n",
        "            print(f\"\\n  Test {i+1}:\")\n",
        "            print(f\"    Prompt: {result['prompt'][:50]}...\")\n",
        "            print(f\"    Quality: {result['quality_score']:.2%}\")\n",
        "            print(f\"    Latency: {result['latency_ms']:.2f}ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost-Performance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create cost-performance scatter plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for model_name in models:\n",
        "    model_results = comparison[\"detailed_results\"][model_name]\n",
        "    ax.scatter(\n",
        "        model_results[\"average_latency_ms\"],\n",
        "        model_results[\"average_quality_score\"],\n",
        "        s=model_results[\"total_cost\"] * 10000,  # Size by cost\n",
        "        alpha=0.6,\n",
        "        label=model_name,\n",
        "    )\n",
        "    ax.annotate(\n",
        "        model_name,\n",
        "        (model_results[\"average_latency_ms\"], model_results[\"average_quality_score\"]),\n",
        "        xytext=(5, 5),\n",
        "        textcoords='offset points',\n",
        "    )\n",
        "\n",
        "ax.set_xlabel(\"Average Latency (ms)\", fontsize=12)\n",
        "ax.set_ylabel(\"Quality Score\", fontsize=12)\n",
        "ax.set_title(\"Cost-Performance Analysis\\n(Bubble size = Cost)\", fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comparison results to JSON\n",
        "with open(\"evaluation_comparison.json\", \"w\") as f:\n",
        "    json.dump(comparison, f, indent=2)\n",
        "\n",
        "# Save to CSV for easy analysis\n",
        "comparison_df = pd.DataFrame({\n",
        "    model: {\n",
        "        \"Quality Score\": comp[\"average_quality_score\"],\n",
        "        \"Latency (ms)\": comp[\"average_latency_ms\"],\n",
        "        \"Cost\": comp[\"total_cost\"],\n",
        "        \"Success Rate\": comp[\"successful\"] / comp[\"total_tests\"],\n",
        "    }\n",
        "    for model, comp in comparison[\"detailed_results\"].items()\n",
        "})\n",
        "\n",
        "comparison_df.T.to_csv(\"evaluation_comparison.csv\")\n",
        "print(\"Results saved to:\")\n",
        "print(\"  - evaluation_comparison.json\")\n",
        "print(\"  - evaluation_comparison.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
